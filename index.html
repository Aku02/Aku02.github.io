<!DOCTYPE HTML>
<html lang="en">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-163784922-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};
        //JS: "||" is OR function, push() is append()
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

gtag('config', 'UA-163784922-1');
    </script>
    <title>Yash Kant</title>
    <meta name="author" content="Yash Kant">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" type="image/png" href="data/yashkant2.jpg">
</head>

<body>
<table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
    <tr style="padding:0px">
        <td style="padding:0px">
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr style="padding:0px">
                    <td style="padding:2.5%;width:63%;vertical-align:middle">
                        <p style="text-align:center">
                            <name>Yash Kant</name>
                        </p>

                        <p> Hi! I am a Ph.D. student in <a href="https://web.cs.toronto.edu/">Department of Computer Science</a> and <a href="https://robotics.cs.toronto.edu/">Robotics Group</a> at University of Toronto. I am advised by <a href="https://www.gilitschenski.org/igor/">Igor Gilitschenski</a>. </p>

                        <p> I am working (part-time) at <a href="https://research.snap.com/team/category/creative-vision">Snap Research</a> in <a href="http://www.stulyakov.com/">Sergey Tulyakov's</a> team. Here, I am trying to build neural representations for deformable 3D objects.</p>

                        <p> Previously, I was a Research Visitor at Georgia Tech advised by <a href="https://www.cc.gatech.edu/~parikh/">Devi Parikh</a> and <a href="https://www.cc.gatech.edu/~dbatra/">Dhruv Batra</a> for two years. There, I built Visual Question Answering models that can <a href="https://yashkant.github.io/projects/sam-textvqa.html">read</a></a> and are <a href="https://yashkant.github.io/projects/concat-vqa.html">robust</a>, and a <a href="https://yashkant.github.io/housekeep/">benchmark</a></a> to measure commonsense in embodied AI agents. </p>


<!--                        <p>I finished my undergraduate studies from <a href="http://iitr.ac.in">IIT Roorkee</a>. I have interned at <a-->
<!--                                href="https://www.microsoft.com/en-in/msidc/bangalore-campus.aspx">Microsoft,-->
<!--                            Bangalore</a> and visited <a href="http://www.nus.edu.sg/">National University of-->
<!--                            Singapore</a> twice as a research assistant.-->
<!--                        <p>-->

                        <p>I enjoy talking to people and building (hopefully useful) things together. :) </p>

                        <p>These days, I also work closely with MS/BS students, and have spot(s) for students with sufficient time and motivation. To get in touch, please send an <a href="mailto:ysh.kant@gmail.com">email</a>!</p>

                        <p style="text-align:center">
                            <a href="mailto:ysh.kant@gmail.com">Email</a> &nbsp/&nbsp
                            <a href="data/cv-yashkant.pdf"
                               onclick="ga('send', 'event', 'Videos', 'play', 'Fall Campaign')">CV</a> &nbsp/&nbsp
                            <a href="https://github.com/yashkant"> Github </a> &nbsp/&nbsp
                            <a href="https://scholar.google.com/citations?user=rwNKTYIAAAAJ&hl=en"> Google Scholar </a> &nbsp/&nbsp
                            <a href="https://twitter.com/yash2kant">Twitter</a> &nbsp/&nbsp
                            <a href="https://in.linkedin.com/in/yash-kant/"> LinkedIn </a>
                        </p>
                    </td>
                    <td style="padding:2.5%;width:40%;max-width:40%">
                        <a href="data/yashkant2.jpg"><img style="width:100%;max-width:100%" alt="profile photo"
                                                         src="data/yashkant2.jpg" class="hoverZoomLink"></a>
                    </td>
                </tr>
                </tbody>
            </table>


<!--            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">-->
<!--                <tbody>-->
<!--                <tr>-->
<!--                    <td style="padding:20px;width:100%;vertical-align:middle">-->
<!--                        <heading>News</heading>-->
<!--                        <ul>-->
<!--                            <li> [Jan 2022] TAing the <u>first iteration</u> of <a href="https://uoft-isl.github.io/csc498-w22/">Robotics Perception</a> w/ my advisor <a href="http://www.gilitschenski.org/igor/">Igor Gilitschenski</a>!</li>-->
<!--                            <li> [Oct 2021] Serving as a reviewer for CVPR, 2022!</li>-->
<!--                            <li> [Sep 2021] TA-ing <a href="http://www.cs.toronto.edu/~florian/courses/csc477_fall21/">Introduction to Mobile Robotics</a> w/ <a href="http://www.cs.toronto.edu/~florian/">Florian Shkurti</a>!</li>-->
<!--                            <li> [Aug 2021] Started my PhD at University of Toronto!</li>-->
<!--                            <li> [Jul 2021] ConClaT (aka ConCAT) accepeted to ICCV, 2021!</li>-->
<!--                            <li> [Feb 2021] One work accepted at-->
<!--                                <a href="https://chi2021.acm.org"> CHI EA, 2021</a></li>-->
<!--                            <li> [Jan 2021] Serving as a reviewer for ICCV, 2021.</li>-->
<!--                            <li> [Oct 2020] ConCAT accepted to <a href="https://sslneuips20.github.io/">NeurIPS 2020 SSL-->
<!--                                workshop!</a></li>-->
<!--                            <li> [Oct 2020] ConCAT on arxiv: <a href="https://arxiv.org/abs/2010.06087">Contrast and-->
<!--                                Classify: Training Robust VQA Models</a></li>-->
<!--                            <li> [Sep 2020] Lead organizer of <a href="https://textvqa.org/challenge">TextVQA</a>-->
<!--                                and co-organizer of <a href="https://textvqa.org/textcaps/challenge">TextCaps</a> and <a-->
<!--                                        href="https://visualqa.org/workshop.html">VQA</a> 2021 challenges.-->
<!--                            <li> [Jul 2020] Runners-up of the <a href="https://textvqa.org/challenge">TextVQA</a>-->
<!--                                challenge organized at <a href="https://visualqa.org/workshop.html">CVPR 2020!</a>-->
<!--                            </li>-->
<!--                            <li> [Jul 2020] SAM accepted to ECCV, 2020 and CVPR 2020 VQA workshop!</a></li>-->
<!--                            <li> [Jul 2020] SAM on arxiv: <a href="https://arxiv.org/abs/2007.12146">Spatially Aware-->
<!--                                Multimodal Transformers for TextVQA</a></li>-->
<!--                            <li> [Jul 2019] Revamped the <a href="http://demo.visualdialog.org/">Visual Chatbot</a>-->
<!--                                with a complete rewrite of the old Lua-Torch codebase.-->
<!--                            </li>-->
<!--                            <li> [May 2019] I will be visiting Georgia Tech working with Devi Parikh and Dhruv Batra.-->
<!--                            </li>-->
<!--                        </ul>-->
<!--                    </td>-->
<!--                </tr>-->
<!--                </tbody>-->
<!--            </table>-->

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding-left:20px;padding-right:20px;padding-top:20px;width:25%;vertical-align:middle">
                        <heading>Research</heading>
                    </td>
                </tr>

                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <div class="two" id="lh_image" style="opacity: 0;">
                                <video muted="" autoplay="" loop="" width="100%" height="100%">
                                    <source src="data/cos-eor/clutter.gif" type="video/mp4">
                                    Your browser does not support the video tag.
                                </video>
                            </div>
                            <img src="data/laterf/relight.png">
                        </div>
                    </td>

                    <td style="padding:20px;width:100%;vertical-align:top">
                        <a href="https://arxiv.org/abs/2207.01583">
                            <papertitle>LaTeRF: Label and Text Driven Object Radiance Fields</papertitle>
                        </a>
                        <br>
                        Ashkan Mirzaei, <strong>Yash Kant</strong>, Jonathan Kelly, and Igor Gilitschenski
                        <br>
                        <em>ECCV, 2022</em>
                        <br>
                        <a href="https://arxiv.org/abs/2207.01583">arXiv</a> /
                        <a href="https://github.com/ashmrz/LaTeRF">code</a>
                        <br> We build a simple method to extract an object from a scene given 2D images, camera poses, a natural language description of the object, and a few annotated pixels of object and background.<br>
                    </td>
                </tr>

                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <div class="two" id="lh_image" style="opacity: 0;">
                                <video muted="" autoplay="" loop="" width="100%" height="100%">
                                    <source src="data/cos-eor/clutter.gif" type="video/mp4">
                                    Your browser does not support the video tag.
                                </video>
                            </div>
                            <img src="data/cos-eor/clutter.gif">
                        </div>
                    </td>

                    <td style="padding:20px;width:100%;vertical-align:top">
                        <a href="https://yashkant.github.io/housekeep">
                            <papertitle>Housekeep: Tidying Virtual Households using Commonsense Reasoning</papertitle>
                        </a>
                        <br>
                        <strong>Yash Kant</strong>, Arun Ramachandran, Sriram Yenamandra, Igor Gilitschenski, Dhruv Batra, Andrew Szot*, and Harsh Agrawal*
                        <br>
                        <em>ECCV, 2022</em>
                        <br>
                        <a href="https://arxiv.org/abs/2205.10712">arXiv</a> /
                        <a href="https://yashkant.github.io/housekeep">project page</a> /
                        <a href="https://github.com/yashkant/housekeep">code</a> /
                        <a href="https://colab.research.google.com/drive/12LweZ0xlGlO_SP4-pz1wbb5tOlXHLZ5R?usp=sharing">colab</a>
                        <br>
                        Housekeep is a benchmark to evaluate commonsense reasoning in the home for embodied AI. Here, an embodied agent must tidy a house by rearranging misplaced objects without explicit instructions.
                        <br>
                        <br>
                        To capture the rich diversity of real world scenarios, we support cluttering environments with ~1800 everyday 3D object models spread across ~270 categories!
                    </td>
                </tr>

                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <div class="two" id="lh_image" style="opacity: 0;">
                                <video muted="" autoplay="" loop="" width="100%" height="100%">
                                    <source src="data/concat-vqa/teaser.gif" type="video/mp4">
                                    Your browser does not support the video tag.
                                </video>
                            </div>
                            <img src="data/concat-vqa/teaser.gif">
                        </div>
                    </td>

                    <td style="padding:20px;width:100%;vertical-align:top">
                        <a href="https://yashkant.github.io/projects/concat-vqa.html">
                            <papertitle>Contrast and Classify: Training Robust VQA Models</papertitle>
                        </a>
                        <br>
                        <strong>Yash Kant</strong>,
                        <a href="https://amoudgl.github.io/">Abhinav Moudgil</a>,
                        <a href="https://www.cc.gatech.edu/~dbatra/">Dhruv Batra</a>,
                        <a href="https://www.cc.gatech.edu/~parikh/">Devi Parikh</a>,
                        <a href="https://dexter1691.github.io/">Harsh Agrawal</a>
                        <br>
                        <em>ICCV, 2021</em>
                        <br>
                        <a href="https://arxiv.org/abs/2010.06087">arXiv</a> /
                        <a href="https://yashkant.github.io/projects/concat-vqa.html">project page</a> /
                        <a href="https://github.com/yashkant/concat-vqa">code</a> /
                        <a href="https://yashkant.github.io/data/concat-vqa/slides.pdf">slides</a>
                        <br>
                        <p>We propose a training scheme which steers VQA models towards answering paraphrased questions consistently, and we ended up beating previous baselines by an absolute 5.8% on consistency metrics without any performance drop!</p>
                    </td>
                </tr>

                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <div class="two" id="lh_image" style="opacity: 0;">
                                <video muted="" autoplay="" loop="" width="100%" height="100%">
                                    <source src="data/sam-textvqa/textvqa-workshop.gif" type="video/mp4">
                                    Your browser does not support the video tag.
                                </video>
                            </div>
                            <img src="data/sam-textvqa/textvqa-workshop.gif">
                        </div>
                    </td>

                    <td style="padding:20px;width:100%;vertical-align:top">
                        <a href="https://yashkant.github.io/projects/sam-textvqa.html">
                            <papertitle>Spatially Aware Multimodal Transformers for TextVQA</papertitle>
                        </a>
                        <br>
                        <strong>Yash Kant</strong>,
                        <a href="https://www.cc.gatech.edu/~dbatra/">Dhruv Batra</a>,
                        <a href="https://panderson.me/">Peter Anderson</a>,
                        <a href="https://www.cc.gatech.edu/~jlu347/">Jiasen Lu</a>,
                        <a href="https://www.alexander-schwing.de/">Alexander Schwing</a>,
                        <a href="https://www.cc.gatech.edu/~parikh/">Devi Parikh</a>,
                        <a href="https://dexter1691.github.io/">Harsh Agrawal</a>
                        <br>
                        <em>ECCV, 2020</em>
                        <br>
                        <a href="https://arxiv.org/abs/2007.12146">arXiv</a> /
                        <a href="https://yashkant.github.io/projects/sam-textvqa.html">project page</a> /
                        <a href="https://github.com/yashkant/sam-textvqa">code</a> /
                        <a href="https://www.youtube.com/watch?v=uPZra6HfLd0">short talk</a> /
                        <a href="https://www.youtube.com/watch?v=rO89lcTvz2U">long talk</a> /
                        <a href="https://yashkant.github.io/data/sam-textvqa/slides.pdf">slides</a>
                        <br>
                        <p>We built a self-attention module to reason over spatial graphs in images. We ended up with an absolute performance improvement of more than 4% on two TextVQA bechmarks! </p>

                    </td>
                </tr>

                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <div class="two" id="lh_image" style="opacity: 0;">
                                <video muted="" autoplay="" loop="" width="100%" height="100%">
                                    <source src="data/concat-vqa/teaser.gif" type="video/mp4">
                                    Your browser does not support the video tag.
                                </video>
                            </div>
                            <img src="data/visdial-on-demand/chi_ea_system.png">
                        </div>
                    </td>

                    <td style="padding:20px;width:100%;vertical-align:top">
                        <a href="https://dl.acm.org/doi/10.1145/3411763.3451810">
                        <papertitle>Automated Video Description for Blind and Low Vision Users</papertitle>
                        </a>
                        <br>
                        Aditya Bodi, Pooyan Fazli, Shasta Ihorn, Yue-Ting Siu, Andrew T Scott, Lothar Narins,
                        <br>
                        <strong>Yash Kant</strong>, Abhishek Das, Ilmi Yoon
                        <br>
                        <em>CHI Extended Abstracts, 2021</em>
                        <br>
                        <a href="https://dl.acm.org/doi/10.1145/3411763.3451810">paper</a>
                                                <p>We built a system to automatically generate descriptions for videos and answer blind and low vision users’ queries on the videos!</p>
                        <br>
                    </td>
                </tr>


                </tbody>
            </table>



<!--            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">-->
<!--                <tbody>-->
<!--                <tr>-->
<!--                    <td style="padding-left:20px;padding-right:20px;padding-top:20px;width:25%;vertical-align:middle">-->
<!--                        <heading>Research</heading>-->
<!--                    </td>-->
<!--                </tr>-->

<!--                <tr>-->
<!--                    <td style="padding:20px;width:25%;vertical-align:middle">-->
<!--                        <div class="one">-->
<!--                            <div class="two" id="lh_image" style="opacity: 0;">-->
<!--                                <video muted="" autoplay="" loop="" width="100%" height="100%">-->
<!--                                    <source src="data/concat-vqa/teaser.gif" type="video/mp4">-->
<!--                                    Your browser does not support the video tag.-->
<!--                                </video>-->
<!--                            </div>-->
<!--                            <img src="data/concat-vqa/teaser.gif">-->
<!--                        </div>-->
<!--                    </td>-->

<!--                    <td style="padding:20px;width:100%;vertical-align:top">-->
<!--                        <a href="https://yashkant.github.io/projects/concat-vqa.html">-->
<!--                            <papertitle>Contrast and Classify: Training Robust VQA Models</papertitle>-->
<!--                        </a>-->
<!--                        <br>-->
<!--                        <strong>Yash Kant</strong>,-->
<!--                        <a href="https://amoudgl.github.io/">Abhinav Moudgil</a>,-->
<!--                        <a href="https://www.cc.gatech.edu/~dbatra/">Dhruv Batra</a>,-->
<!--                        <a href="https://www.cc.gatech.edu/~parikh/">Devi Parikh</a>,-->
<!--                        <a href="https://dexter1691.github.io/">Harsh Agrawal</a>-->
<!--                        <br>-->
<!--                        <em>International Conference on Computer Vision (ICCV), 2021</em>-->
<!--                        <br>-->
<!--                        <em> Self-Supervised Learning Workshop at NeurIPS, 2020</em>-->
<!--                        <br>-->
<!--                        <a href="https://arxiv.org/abs/2010.06087">arXiv</a> /-->
<!--                        <a href="https://yashkant.github.io/projects/concat-vqa.html">project page</a> /-->
<!--                        <a href="https://github.com/yashkant/concat-vqa">code</a> /-->
<!--                        <a href="https://yashkant.github.io/data/concat-vqa/slides.pdf">slides</a>-->
<!--                        <br>-->
<!--                        <p>We propose a training scheme which steers VQA models towards answering paraphrased questions consistently, and we ended up beating previous baselines by an absolute 5.8% on consistency metrics without any performance drop!</p>-->
<!--                    </td>-->
<!--                </tr>-->

<!--                <tr>-->
<!--                    <td style="padding:20px;width:25%;vertical-align:middle">-->
<!--                        <div class="one">-->
<!--                            <div class="two" id="lh_image" style="opacity: 0;">-->
<!--                                <video muted="" autoplay="" loop="" width="100%" height="100%">-->
<!--                                    <source src="data/sam-textvqa/textvqa-workshop.gif" type="video/mp4">-->
<!--                                    Your browser does not support the video tag.-->
<!--                                </video>-->
<!--                            </div>-->
<!--                            <img src="data/sam-textvqa/textvqa-workshop.gif">-->
<!--                        </div>-->
<!--                    </td>-->

<!--                    <td style="padding:20px;width:100%;vertical-align:top">-->
<!--                        <a href="https://yashkant.github.io/projects/sam-textvqa.html">-->
<!--                            <papertitle>Spatially Aware Multimodal Transformers for TextVQA</papertitle>-->
<!--                        </a>-->
<!--                        <br>-->
<!--                        <strong>Yash Kant</strong>,-->
<!--                        <a href="https://www.cc.gatech.edu/~dbatra/">Dhruv Batra</a>,-->
<!--                        <a href="https://panderson.me/">Peter Anderson</a>,-->
<!--                        <a href="https://www.cc.gatech.edu/~jlu347/">Jiasen Lu</a>,-->
<!--                        <a href="https://www.alexander-schwing.de/">Alexander Schwing</a>,-->
<!--                        <a href="https://www.cc.gatech.edu/~parikh/">Devi Parikh</a>,-->
<!--                        <a href="https://dexter1691.github.io/">Harsh Agrawal</a>-->
<!--                        <br>-->
<!--                        <em>European Conference on Computer Vision (ECCV), 2020</em>-->
<!--                        <br>-->
<!--                        <em>VQA Workshop at CVPR, 2020</em>-->
<!--                        <br>-->
<!--                        <a href="https://arxiv.org/abs/2007.12146">arXiv</a> /-->
<!--                        <a href="https://yashkant.github.io/projects/sam-textvqa.html">project page</a> /-->
<!--                        <a href="https://github.com/yashkant/sam-textvqa">code</a> /-->
<!--                        <a href="https://www.youtube.com/watch?v=uPZra6HfLd0">short talk</a> /-->
<!--                        <a href="https://www.youtube.com/watch?v=rO89lcTvz2U">long talk</a> /-->
<!--                        <a href="https://yashkant.github.io/data/sam-textvqa/slides.pdf">slides</a>-->
<!--                        <br>-->
<!--                        <p>We built a self-attention module to reason over spatial graphs in images. We ended up with an absolute performance improvement of more than 4% on two TextVQA bechmarks! </p>-->

<!--                    </td>-->
<!--                </tr>-->

<!--                <tr>-->
<!--                    <td style="padding:20px;width:25%;vertical-align:middle">-->
<!--                        <div class="one">-->
<!--                            <div class="two" id="lh_image" style="opacity: 0;">-->
<!--                                <video muted="" autoplay="" loop="" width="100%" height="100%">-->
<!--                                    <source src="data/concat-vqa/teaser.gif" type="video/mp4">-->
<!--                                    Your browser does not support the video tag.-->
<!--                                </video>-->
<!--                            </div>-->
<!--                            <img src="data/visdial-on-demand/chi_ea_system.png">-->
<!--                        </div>-->
<!--                    </td>-->

<!--                    <td style="padding:20px;width:100%;vertical-align:top">-->
<!--                        <a href="https://dl.acm.org/doi/10.1145/3411763.3451810">-->
<!--                        <papertitle>Automated Video Description for Blind and Low Vision Users</papertitle>-->
<!--                        </a>-->
<!--                        <br>-->
<!--                        Aditya Bodi, Pooyan Fazli, Shasta Ihorn, Yue-Ting Siu, Andrew T Scott, Lothar Narins,-->
<!--                        <br>-->
<!--                        <strong>Yash Kant</strong>, Abhishek Das, Ilmi Yoon-->
<!--                        <br>-->
<!--                        <em>CHI Extended Abstracts 2021</em>-->
<!--                        <br>-->
<!--                        <a href="https://dl.acm.org/doi/10.1145/3411763.3451810">paper</a>-->
<!--                                                <p>We built a system to automatically generate descriptions for videos and answer blind and low vision users’ queries on the videos!</p>-->
<!--                        <br>-->
<!--                    </td>-->
<!--                </tr>-->


<!--                </tbody>-->
<!--            </table>-->




            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:20px;width:100%;vertical-align:middle">
                        <heading>Projects</heading>
                        <p class="content">
                        <ul>
                            <li><b><i>Adding Complement Objective Training to Pythia:</i></b> I experimented with adding
                                Complement Objective Training in FAIR's vision and language framework <a
                                        href="https://github.com/facebookresearch/pythia/">Pythia</a> and also wrote a
                                report on my findings <a
                                        href="https://drive.google.com/file/d/16NtLvZvBPq1cRVeCSq7sXXg0C8NkSi4l/view">here</a>,
                                the code is <a href="https://github.com/facebookresearch/pythia/pull/32">here</a>.
                            </li>
                            <li><b><i>ICLR Reproducibility Challenge:</i></b> We reproduced <a
                                    href="https://arxiv.org/abs/1806.06763">Closing the Generalization Gap of Adaptive
                                Gradient Methods in Training Deep Neural Networks </a> and here's the <a
                                    href="https://github.com/yashkant/Padam-Tensorflow">code.</a></li>
                            <li><b><i>Visual Chatbot Version 2.0 (<a
                                    href="https://github.com/Cloud-CV/visual-chatbot/pull/18">code here</a>):</i></b> I
                                shifted the old Lua-Torch codebase to PyTorch, added better captioning and trained the
                                VisDial model on BUTD features.
                            </li>
                            <li><b><i>Quantized Neural Architecture Search:</i></b> I quantized the search
                                space of Neural Architecture Search algorithms (<a
                                        href="https://arxiv.org/abs/1802.03268">ENAS</a>, 
                                    <a href="https://arxiv.org/abs/1712.00559">PNAS</a>) to search for
                                resource-efficient models. Code is <a href="https://github.com/yashkant/ENAS-Quantized-Neural-Networks">here (ENAS)</a>, 
                                and <a href="https://github.com/yashkant/PNAS-Binarized-Neural-Networks">here (PNAS).</a>
                            </li>
                        </ul>
                        </p>
                    </td>
                </tr>
                </tbody>
            </table>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:20px;width:100%;vertical-align:middle">
                        <heading>Fun</heading>
                        <p class="content">
                        <ul>
                            <li> Inspired by <a href="http://abhishekdas.com/">@abhskdz's</a> webpage, I got myself a <a href="https://conquer.earth/yashkant">conquer.earth</a> account.</li>
                        </ul>
                        </p>
                    </td>
                </tr>
                </tbody>
            </table>


            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:0px">
                        <br>
                        <p style="text-align:right;font-size:small;">
                            I borrowed this template from Jon Barron's<a
                                href="https://github.com/jonbarron/jonbarron_website"> website</a>.
                            <br>
                        </p>
                    </td>
                </tr>
                </tbody>
            </table>
        </td>
    </tr>
</table>
</body>

</html>
