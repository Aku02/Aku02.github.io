<!DOCTYPE HTML>
<html lang="en">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-163784922-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};
        //JS: "||" is OR function, push() is append()
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

gtag('config', 'UA-163784922-1');
    </script>
    <title>Akash </title>
    <meta name="author" content="Akash ">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" type="image/jpg" href="data/akash2.jpg">
</head>

<body>
<table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
    <tr style="padding:0px">
        <td style="padding:0px">
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr style="padding:0px">
                    <td style="padding:2.5%;width:63%;vertical-align:middle">
                        <p style="text-align:center">
                            <name>Akash </name>
                        </p>

                        <p> Hello There! I'm an undergrad <a href="https://tce.edu/">@TCE</a> pursuing Mechanical Engineering.</p>

                        <p>I'm just another human in planet EARTH C-137. :) </p>

                        <p><a href="mailto:kaakash11c@gmail.com">Drop a message here!</a>!</p>

                        <p style="text-align:center">
                            <a href="mailto:kaakash11c@gmail.com">Email</a> &nbsp/&nbsp
                            <a href="data/cv-akash.pdf"
                               onclick="ga('send', 'event', 'Videos', 'play', 'Fall Campaign')">CV</a> &nbsp/&nbsp
                            <a href="https://github.com/Aku02"> Github </a> &nbsp/&nbsp
                            <a href="https://scholar.google.com/citations?user=RyZLpyIAAAAJ&hl=en"> Google Scholar </a> &nbsp/&nbsp
                            <a href="https://www.linkedin.com/in/akash-k-737884192/"> LinkedIn </a>
                        </p>
                    </td>
                    <td style="padding:2.5%;width:40%;max-width:40%">
                        <a href="data/akash2.jpg"><img style="width:100%;max-width:100%" alt="profile photo"
                                                         src="data/akash2.jpg" class="hoverZoomLink"></a>
                    </td>
                </tr>
                </tbody>
            </table>




            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding-left:20px;padding-right:20px;padding-top:20px;width:25%;vertical-align:middle">
                        <heading>Research</heading>
                    </td>
                </tr>

                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <div class="two" id="lh_image" style="opacity: 0;">
                                <video muted="" autoplay="" loop="" width="100%" height="100%">
                                    <source src="data/cos-eor/iiwa.gif" type="video/mp4">
                                    Your browser does not support the video tag.
                                </video>
                            </div>
                            <img src="data/cos-eor/iiwa.gif">
                        </div>
                    </td>

                    <td style="padding:20px;width:100%;vertical-align:top">
                        <a href="https://arxiv.org/abs/2207.01583">
                            <papertitle>Building skeletonized articulated models from casual videos</papertitle>
                        </a>
                        <br>
                        <strong>Manuscript in preparation</strong>
                        <br>
                        <!-- <em>ECCV, 2022</em> -->
                        <br>
                        <a href="https://arxiv.org/abs/2112.12761">arXiv</a> /
                        <a href="https://docs.google.com/presentation/d/e/2PACX-1vRBPKVI6uOB22zxdaau1E4KmFhPMeL5P_ZzBSoiUrsfOTTwdKzFV7dHpSikbpD2DMA45-5x23BI12ms/pub?start=false&loop=false&delayms=3000">code</a>
                        <br> Kinematic aware, Animatable and Class-agnostic 3D deformable objects from causual monocular videos without the use any priors or camera poses - <strong>A Template free approach</strong> <br>
                        <br> <strong>Keywords: NeRF , Nerual Blend Skinning , Camera pose optimisation</strong> <br>
                    </td>
                </tr>

                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <div class="two" id="lh_image" style="opacity: 0;">
                                <video muted="" autoplay="" loop="" width="100%" height="100%">
                                    <source src="data/lyft/lyft.gif" type="video/mp4">
                                    Your browser does not support the video tag.
                                </video>
                            </div>
                            <img src="data/lyft/lyft.gif">
                        </div>
                    </td>

                    <td style="padding:20px;width:100%;vertical-align:top">
                        <a href="https://akash.github.io/housekeep">
                            <papertitle>Lyft L5 toolkit - <strong>Motion forcasting</strong> </strong></papertitle>
                        </a>
                        <br>
                        <em>Kaggle - Build motion prediction models for self-driving vehicles | 2021</em>
                        <br>
                        <!-- <a href="https://arxiv.org/abs/2205.10712">arXiv</a> / -->
                        <a href="https://docs.google.com/presentation/d/e/2PACX-1vSwqeuDqgy_TqODsvNGKlJ32XHan7UcPLeJa0X21imF1T3fk5rb_g8c2Xhe_yk4yePWjuvveijXyFed/pub?start=false&loop=false&delayms=1000&slide=id.p">Model Architecture</a> /
                        <a href="https://gitfront.io/r/user-7662064/yTzSmPmkkuYB/vectornet-working/">code</a> /
                        <!-- <a href="https://colab.research.google.com/drive/12LweZ0xlGlO_SP4-pz1wbb5tOlXHLZ5R?usp=sharing">colab</a> -->
                        <br>
                        Motion prediction with the help of rasterized set of images, CNN , LSTM encoder-decoder based model. 
                        <br>
                        <br>
                        Also, currenly exploring a Vectornet based approach skipping the whole razterization part and making the pipeline faster.
                    </td>
                </tr>

                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <div class="two" id="lh_image" style="opacity: 0;">
                                <video muted="" autoplay="" loop="" width="100%" height="100%">
                                    <source src="data/anno/nfl.gif" type="video/mp4">
                                    Your browser does not support the video tag.
                                </video>
                            </div>
                            <img src="data/anno/nfl.gif">
                        </div>
                    </td>

                    <td style="padding:20px;width:100%;vertical-align:top">
                        <a href="https://gitfront.io/r/user-7662064/1csDFNTX7w7M/FairMOT/">
                            <papertitle>Segment and label helmets in video footage</papertitle>
                        </a>
                        <br>
                        <em>Kaggle - NFL, 2021</em>
                        <br>
                        <!-- <a href="https://arxiv.org/abs/2010.06087">arXiv</a> / -->
                        <a href="https://docs.google.com/presentation/d/e/2PACX-1vTOKESkB1xbv3_IKvDgmN3UZiTgd5LOIM19-kxLes3fIt0titlB2MvVaf0E-qxCUChcBsMSLtXNrHhd/pub?start=true&loop=false&delayms=60000">results</a> /
                        <a href="https://gitfront.io/r/user-7662064/1csDFNTX7w7M/FairMOT/">code</a> /
                        <!-- <a href="https://akash.github.io/data/lyft/slides.pdf">slides</a> -->
                        <br>
                        <p> • Detector to find helmets, Image2Map (BEV) \\
                            • Classifier to classify players into 2(H/V) teams and Registration of detected players on 2D map to provided tracking data. Later track detected \\
                            bounding boxes and reassign players. \\ 
                            • Predict the 2022 College Men’s Basketball Tournament \\
                            • Analyse the trend based on past 5 year’s data</p>
                    </td>
                </tr>

                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <div class="two" id="lh_image" style="opacity: 0;">
                                <video muted="" autoplay="" loop="" width="100%" height="100%">
                                    <source src="data/anno/annno.gif" type="video/mp4">
                                    Your browser does not support the video tag.
                                </video>
                            </div>
                            <img src="data/anno/annno.gif">
                        </div>
                    </td>

                    <td style="padding:20px;width:100%;vertical-align:top">
                        <a href="https://akash.github.io/projects/sam-textvqa.html">
                            <papertitle>Automated Annotation and Classification of Catheters in Chest X-Rays</papertitle>
                        </a>
                        <br>
                        <strong>Akash </strong>,
                        <a href="https://scholar.google.co.in/citations?user=_cpaQAgAAAAJ&hl=en">Saravana Perumaal Subramanian</a>,
                        <br>
                        <em>ICCCSP, 2022</em>
                        <br>
                        <a href="https://doi.org/10.1007/978-3-031-11633-9_12">paper</a> /
                        <!-- <a href="https://akash.github.io/projects/sam-textvqa.html">project page</a> / -->
                        <a href="https://www.kaggle.com/ak0210/datagen-chromagan">data</a> /
                        <!-- <a href="https://www.youtube.com/watch?v=uPZra6HfLd0">short talk</a> / -->
                        <!-- <a href="https://www.youtube.com/watch?v=rO89lcTvz2U">long talk</a> / -->
                        <!-- <a href="https://akash.github.io/data/sam-textvqa/slides.pdf">slides</a> -->
                        <br>
                        <p>Annotate and classify cathater position. Robust approach to get the endpoints of the cathaters even though the enpoints contribute to a less than 1% footprint of the whole image </p>

                    </td>
                </tr>

                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <div class="two" id="lh_image" style="opacity: 0;">
                                <video muted="" autoplay="" loop="" width="100%" height="100%">
                                    <source src="data/lyft/teaser.gif" type="video/mp4">
                                    Your browser does not support the video tag.
                                </video>
                            </div>
                            <img src="data/visdial-on-demand/chi_ea_system.png">
                        </div>
                    </td>

                    <td style="padding:20px;width:100%;vertical-align:top">
                        <a href="https://dl.acm.org/doi/10.1145/3411763.3451810">
                        <papertitle>Automated Video Description for Blind and Low Vision Users</papertitle>
                        </a>
                        <br>
                        Aditya Bodi, Pooyan Fazli, Shasta Ihorn, Yue-Ting Siu, Andrew T Scott, Lothar Narins,
                        <br>
                        <strong>Akash </strong>, Abhishek Das, Ilmi Yoon
                        <br>
                        <em>CHI Extended Abstracts, 2021</em>
                        <br>
                        <a href="https://dl.acm.org/doi/10.1145/3411763.3451810">paper</a>
                                                <p>We built a system to automatically generate descriptions for videos and answer blind and low vision users’ queries on the videos!</p>
                        <br>
                    </td>
                </tr>


                </tbody>
            </table>



<!--            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">-->
<!--                <tbody>-->
<!--                <tr>-->
<!--                    <td style="padding-left:20px;padding-right:20px;padding-top:20px;width:25%;vertical-align:middle">-->
<!--                        <heading>Research</heading>-->
<!--                    </td>-->
<!--                </tr>-->

<!--                <tr>-->
<!--                    <td style="padding:20px;width:25%;vertical-align:middle">-->
<!--                        <div class="one">-->
<!--                            <div class="two" id="lh_image" style="opacity: 0;">-->
<!--                                <video muted="" autoplay="" loop="" width="100%" height="100%">-->
<!--                                    <source src="data/lyft/teaser.gif" type="video/mp4">-->
<!--                                    Your browser does not support the video tag.-->
<!--                                </video>-->
<!--                            </div>-->
<!--                            <img src="data/lyft/teaser.gif">-->
<!--                        </div>-->
<!--                    </td>-->

<!--                    <td style="padding:20px;width:100%;vertical-align:top">-->
<!--                        <a href="https://akash.github.io/projects/concat-vqa.html">-->
<!--                            <papertitle>Contrast and Classify: Training Robust VQA Models</papertitle>-->
<!--                        </a>-->
<!--                        <br>-->
<!--                        <strong>Akash </strong>,-->
<!--                        <a href="https://amoudgl.github.io/">Abhinav Moudgil</a>,-->
<!--                        <a href="https://www.cc.gatech.edu/~dbatra/">Dhruv Batra</a>,-->
<!--                        <a href="https://www.cc.gatech.edu/~parikh/">Devi Parikh</a>,-->
<!--                        <a href="https://dexter1691.github.io/">Harsh Agrawal</a>-->
<!--                        <br>-->
<!--                        <em>International Conference on Computer Vision (ICCV), 2021</em>-->
<!--                        <br>-->
<!--                        <em> Self-Supervised Learning Workshop at NeurIPS, 2020</em>-->
<!--                        <br>-->
<!--                        <a href="https://arxiv.org/abs/2010.06087">arXiv</a> /-->
<!--                        <a href="https://akash.github.io/projects/concat-vqa.html">project page</a> /-->
<!--                        <a href="https://github.com/akash/concat-vqa">code</a> /-->
<!--                        <a href="https://akash.github.io/data/lyft/slides.pdf">slides</a>-->
<!--                        <br>-->
<!--                        <p>We propose a training scheme which steers VQA models towards answering paraphrased questions consistently, and we ended up beating previous baselines by an absolute 5.8% on consistency metrics without any performance drop!</p>-->
<!--                    </td>-->
<!--                </tr>-->

<!--                <tr>-->
<!--                    <td style="padding:20px;width:25%;vertical-align:middle">-->
<!--                        <div class="one">-->
<!--                            <div class="two" id="lh_image" style="opacity: 0;">-->
<!--                                <video muted="" autoplay="" loop="" width="100%" height="100%">-->
<!--                                    <source src="data/sam-textvqa/textvqa-workshop.gif" type="video/mp4">-->
<!--                                    Your browser does not support the video tag.-->
<!--                                </video>-->
<!--                            </div>-->
<!--                            <img src="data/sam-textvqa/textvqa-workshop.gif">-->
<!--                        </div>-->
<!--                    </td>-->

<!--                    <td style="padding:20px;width:100%;vertical-align:top">-->
<!--                        <a href="https://akash.github.io/projects/sam-textvqa.html">-->
<!--                            <papertitle>Spatially Aware Multimodal Transformers for TextVQA</papertitle>-->
<!--                        </a>-->
<!--                        <br>-->
<!--                        <strong>Akash </strong>,-->
<!--                        <a href="https://www.cc.gatech.edu/~dbatra/">Dhruv Batra</a>,-->
<!--                        <a href="https://panderson.me/">Peter Anderson</a>,-->
<!--                        <a href="https://www.cc.gatech.edu/~jlu347/">Jiasen Lu</a>,-->
<!--                        <a href="https://www.alexander-schwing.de/">Alexander Schwing</a>,-->
<!--                        <a href="https://www.cc.gatech.edu/~parikh/">Devi Parikh</a>,-->
<!--                        <a href="https://dexter1691.github.io/">Harsh Agrawal</a>-->
<!--                        <br>-->
<!--                        <em>European Conference on Computer Vision (ECCV), 2020</em>-->
<!--                        <br>-->
<!--                        <em>VQA Workshop at CVPR, 2020</em>-->
<!--                        <br>-->
<!--                        <a href="https://arxiv.org/abs/2007.12146">arXiv</a> /-->
<!--                        <a href="https://akash.github.io/projects/sam-textvqa.html">project page</a> /-->
<!--                        <a href="https://github.com/akash/sam-textvqa">code</a> /-->
<!--                        <a href="https://www.youtube.com/watch?v=uPZra6HfLd0">short talk</a> /-->
<!--                        <a href="https://www.youtube.com/watch?v=rO89lcTvz2U">long talk</a> /-->
<!--                        <a href="https://akash.github.io/data/sam-textvqa/slides.pdf">slides</a>-->
<!--                        <br>-->
<!--                        <p>We built a self-attention module to reason over spatial graphs in images. We ended up with an absolute performance improvement of more than 4% on two TextVQA bechmarks! </p>-->

<!--                    </td>-->
<!--                </tr>-->

<!--                <tr>-->
<!--                    <td style="padding:20px;width:25%;vertical-align:middle">-->
<!--                        <div class="one">-->
<!--                            <div class="two" id="lh_image" style="opacity: 0;">-->
<!--                                <video muted="" autoplay="" loop="" width="100%" height="100%">-->
<!--                                    <source src="data/lyft/teaser.gif" type="video/mp4">-->
<!--                                    Your browser does not support the video tag.-->
<!--                                </video>-->
<!--                            </div>-->
<!--                            <img src="data/visdial-on-demand/chi_ea_system.png">-->
<!--                        </div>-->
<!--                    </td>-->

<!--                    <td style="padding:20px;width:100%;vertical-align:top">-->
<!--                        <a href="https://dl.acm.org/doi/10.1145/3411763.3451810">-->
<!--                        <papertitle>Automated Video Description for Blind and Low Vision Users</papertitle>-->
<!--                        </a>-->
<!--                        <br>-->
<!--                        Aditya Bodi, Pooyan Fazli, Shasta Ihorn, Yue-Ting Siu, Andrew T Scott, Lothar Narins,-->
<!--                        <br>-->
<!--                        <strong>Akash </strong>, Abhishek Das, Ilmi Yoon-->
<!--                        <br>-->
<!--                        <em>CHI Extended Abstracts 2021</em>-->
<!--                        <br>-->
<!--                        <a href="https://dl.acm.org/doi/10.1145/3411763.3451810">paper</a>-->
<!--                                                <p>We built a system to automatically generate descriptions for videos and answer blind and low vision users’ queries on the videos!</p>-->
<!--                        <br>-->
<!--                    </td>-->
<!--                </tr>-->


<!--                </tbody>-->
<!--            </table>-->




            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:20px;width:100%;vertical-align:middle">
                        <heading>Projects</heading>
                        <p class="content">
                        <ul>
                            <li><b><i>Adding Complement Objective Training to Pythia:</i></b> I experimented with adding
                                Complement Objective Training in FAIR's vision and language framework <a
                                        href="https://github.com/facebookresearch/pythia/">Pythia</a> and also wrote a
                                report on my findings <a
                                        href="https://drive.google.com/file/d/16NtLvZvBPq1cRVeCSq7sXXg0C8NkSi4l/view">here</a>,
                                the code is <a href="https://github.com/facebookresearch/pythia/pull/32">here</a>.
                            </li>
                            <li><b><i>ICLR Reproducibility Challenge:</i></b> We reproduced <a
                                    href="https://arxiv.org/abs/1806.06763">Closing the Generalization Gap of Adaptive
                                Gradient Methods in Training Deep Neural Networks </a> and here's the <a
                                    href="https://github.com/akash/Padam-Tensorflow">code.</a></li>
                            <li><b><i>Visual Chatbot Version 2.0 (<a
                                    href="https://github.com/Cloud-CV/visual-chatbot/pull/18">code here</a>):</i></b> I
                                shifted the old Lua-Torch codebase to PyTorch, added better captioning and trained the
                                VisDial model on BUTD features.
                            </li>
                            <li><b><i>Quantized Neural Architecture Search:</i></b> I quantized the search
                                space of Neural Architecture Search algorithms (<a
                                        href="https://arxiv.org/abs/1802.03268">ENAS</a>, 
                                    <a href="https://arxiv.org/abs/1712.00559">PNAS</a>) to search for
                                resource-efficient models. Code is <a href="https://github.com/akash/ENAS-Quantized-Neural-Networks">here (ENAS)</a>, 
                                and <a href="https://github.com/akash/PNAS-Binarized-Neural-Networks">here (PNAS).</a>
                            </li>
                        </ul>
                        </p>
                    </td>
                </tr>
                </tbody>
            </table>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:20px;width:100%;vertical-align:middle">
                        <heading>Fun</heading>
                        <p class="content">
                        <ul>
                            <li> Inspired by <a href="http://abhishekdas.com/">@abhskdz's</a> webpage, I got myself a <a href="https://conquer.earth/akash">conquer.earth</a> account.</li>
                        </ul>
                        </p>
                    </td>
                </tr>
                </tbody>
            </table>


            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:0px">
                        <br>
                        <p style="text-align:right;font-size:small;">
                            I borrowed this template from Jon Barron's<a
                                href="https://github.com/jonbarron/jonbarron_website"> website</a>.
                            <br>
                        </p>
                    </td>
                </tr>
                </tbody>
            </table>
        </td>
    </tr>
</table>
</body>

</html>
